{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed4da4d",
   "metadata": {},
   "source": [
    "## Quickstart Guide\n",
    "https://langchain.readthedocs.io/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05530b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (0.0.75)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.21.5)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from pydantic<2,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b556fc2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (0.26.3)\n",
      "Requirement already satisfied: tqdm in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c07fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126a6a1",
   "metadata": {},
   "source": [
    "# Building A Language Model Application\n",
    "### LLMS: Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bef95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3596e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86697b16",
   "metadata": {},
   "source": [
    "The `temperature` parameter in language models like OpenAI's GPT affects the randomness of the model's output. It is a hyperparameter used to control the creativity and diversity of the generated text. Here's how it works:\n",
    "\n",
    "### Understanding Temperature:\n",
    "\n",
    "- **Low Temperature (e.g., 0.1):**\n",
    "  - The model's output will be more deterministic and focused. It will favor the most probable next words and produce less varied and more predictable responses.\n",
    "  - Example: If you ask for a factual answer, a low temperature will help the model stick closely to the most likely correct response.\n",
    "\n",
    "- **High Temperature (e.g., 0.9):**\n",
    "  - The model's output will be more random and creative. It will allow for a broader range of possible next words, leading to more varied and imaginative responses.\n",
    "  - Example: If you are looking for creative writing or brainstorming ideas, a higher temperature will encourage the model to explore more diverse possibilities.\n",
    "\n",
    "### How Temperature Works:\n",
    "\n",
    "The temperature parameter scales the logits (raw predictions) before applying the softmax function to convert them into probabilities. The logits are divided by the temperature value, and then the softmax function is applied:\n",
    "\n",
    "- **Temperature = 1:** The logits are not scaled, and the model's predictions are based on the original probabilities.\n",
    "- **Temperature < 1:** The logits are scaled up, making the probability distribution sharper (more peaked), leading to less randomness.\n",
    "- **Temperature > 1:** The logits are scaled down, making the probability distribution flatter, leading to more randomness.\n",
    "\n",
    "\n",
    "\n",
    "In this example, setting the temperature to 0.9 will encourage the model to produce a more varied and creative continuation of the prompt \"Once upon a time.\"\n",
    "\n",
    "By adjusting the temperature, you can control the balance between creativity and coherence in the generated text, making it a useful tool for various applications, from generating code to writing poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebe6d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy \n",
      "2. Bologna, Italy \n",
      "3. Venice, Italy\n",
      "4. Amalfi Coast, Italy\n",
      "5. Sicily, Italy\n"
     ]
    }
   ],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657e1e9",
   "metadata": {},
   "source": [
    "### Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168f2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6d3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"food\"],\n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160f8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat dessert?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=\"dessert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4309f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. New York City, USA\n",
      "2. Tokyo, Japan\n",
      "3. Paris, France\n",
      "4. San Francisco, USA\n",
      "5. Vancouver, Canada\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=\"dessert\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddba5a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "LangChain is a framework for developing applications with large language models (LLMs). It provides tools for managing prompts, which are crucial for directing LLMs to generate desired outputs. Managing prompts effectively can significantly enhance the performance and usability of LLMs in various applications. Here’s how you can manage prompts for LLMs using LangChain:\n",
    "\n",
    "### Prompt Templates in LangChain\n",
    "\n",
    "**Prompt templates** are predefined templates that can be filled with dynamic inputs to generate specific prompts for an LLM. These templates help standardize the interaction with the model, making it easier to ensure consistency and reproducibility in the responses.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Template Definition**: Define the structure of your prompts.\n",
    "2. **Dynamic Inputs**: Insert variables into your templates.\n",
    "3. **Prompt Management**: Tools to manage and use these templates efficiently.\n",
    "\n",
    "### Example Implementation\n",
    "\n",
    "#### Step 1: Install LangChain\n",
    "\n",
    "First, you need to install the LangChain package if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "#### Step 2: Define a Prompt Template\n",
    "\n",
    "Define a prompt template using LangChain. Here’s a simple example where we want the model to write an article:\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Define the template\n",
    "template = \"\"\"\n",
    "Write a detailed article about {topic}. Make sure to include the following points:\n",
    "1. Introduction to {topic}\n",
    "2. Key features and benefits of {topic}\n",
    "3. Examples or case studies related to {topic}\n",
    "4. Conclusion\n",
    "\n",
    "The article should be informative and engaging.\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt_template = PromptTemplate(template=template)\n",
    "```\n",
    "\n",
    "#### Step 3: Create Dynamic Prompts\n",
    "\n",
    "You can fill this template with dynamic data to generate specific prompts:\n",
    "\n",
    "```python\n",
    "# Dynamic input for the template\n",
    "prompt = prompt_template.format(topic=\"Artificial Intelligence\")\n",
    "\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Write a detailed article about Artificial Intelligence. Make sure to include the following points:\n",
    "1. Introduction to Artificial Intelligence\n",
    "2. Key features and benefits of Artificial Intelligence\n",
    "3. Examples or case studies related to Artificial Intelligence\n",
    "4. Conclusion\n",
    "\n",
    "The article should be informative and engaging.\n",
    "```\n",
    "\n",
    "#### Step 4: Use the Prompt with an LLM\n",
    "\n",
    "Now, you can use this prompt with an LLM. Assuming you have a configured LLM (e.g., using OpenAI's API):\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Generate the article using the prompt\n",
    "response = llm.generate(prompt)\n",
    "\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### Managing Multiple Prompts\n",
    "\n",
    "LangChain allows you to manage multiple prompts easily. You can define and store various templates for different tasks, ensuring that you have a structured approach to interacting with the LLM:\n",
    "\n",
    "```python\n",
    "# Define multiple templates\n",
    "templates = {\n",
    "    \"article\": PromptTemplate(template=\"Write an article about {topic}.\"),\n",
    "    \"summary\": PromptTemplate(template=\"Summarize the following text: {text}\")\n",
    "}\n",
    "\n",
    "# Use a specific template\n",
    "prompt = templates[\"article\"].format(topic=\"Machine Learning\")\n",
    "response = llm.generate(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### Benefits of Using LangChain for Prompt Management\n",
    "\n",
    "1. **Consistency**: Ensures that prompts are consistent across different uses and users.\n",
    "2. **Efficiency**: Saves time by reusing predefined templates.\n",
    "3. **Flexibility**: Easily modify templates and manage dynamic inputs.\n",
    "4. **Scalability**: Manage multiple templates for various tasks and applications.\n",
    "\n",
    "By using LangChain to manage prompts, you can streamline your interactions with large language models, ensuring that you get the most relevant and high-quality responses for your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0c523",
   "metadata": {},
   "source": [
    "### Chains: Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34ff024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffbdf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"food\"],\n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7efda77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b08b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Costa Rica \n",
      "2. Hawaii \n",
      "3. Malaysia \n",
      "4. India \n",
      "5. Thailand\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32dda3",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "\n",
    "### Chains in LangChain\n",
    "\n",
    "Chains in LangChain allow you to create multi-step workflows by combining LLMs with various prompts and functions. These workflows can be used to perform complex tasks by breaking them down into smaller, manageable steps. Each step in the chain can use the output of the previous step as its input, enabling you to build sophisticated applications.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Chain Definition**: Define a series of steps that make up the workflow.\n",
    "2. **Step Functions**: Each step in the chain performs a specific function, which can involve prompting an LLM or other operations.\n",
    "3. **Chain Execution**: Execute the chain to carry out the multi-step workflow.\n",
    "\n",
    "### Example Implementation\n",
    "\n",
    "#### Step 1: Install LangChain\n",
    "\n",
    "First, install the LangChain package if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "#### Step 2: Define Step Functions\n",
    "\n",
    "Define individual functions that will be used as steps in your chain. For example, let’s create a simple chain to write an article and then summarize it.\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.chains import Chain\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Define the article writing function\n",
    "def write_article(topic):\n",
    "    template = \"\"\"\n",
    "    Write a detailed article about {topic}. Make sure to include the following points:\n",
    "    1. Introduction to {topic}\n",
    "    2. Key features and benefits of {topic}\n",
    "    3. Examples or case studies related to {topic}\n",
    "    4. Conclusion\n",
    "\n",
    "    The article should be informative and engaging.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(template=template)\n",
    "    prompt = prompt_template.format(topic=topic)\n",
    "    return llm.generate(prompt)\n",
    "\n",
    "# Define the summarization function\n",
    "def summarize_text(text):\n",
    "    template = \"\"\"\n",
    "    Summarize the following text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(template=template)\n",
    "    prompt = prompt_template.format(text=text)\n",
    "    return llm.generate(prompt)\n",
    "```\n",
    "\n",
    "#### Step 3: Define the Chain\n",
    "\n",
    "Combine these functions into a multi-step workflow using LangChain.\n",
    "\n",
    "```python\n",
    "# Define the chain\n",
    "class ArticleSummaryChain(Chain):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        topic = inputs[\"topic\"]\n",
    "        \n",
    "        # Step 1: Write the article\n",
    "        article = write_article(topic)\n",
    "        \n",
    "        # Step 2: Summarize the article\n",
    "        summary = summarize_text(article)\n",
    "        \n",
    "        return {\"article\": article, \"summary\": summary}\n",
    "\n",
    "# Initialize the chain\n",
    "article_summary_chain = ArticleSummaryChain(llm=llm)\n",
    "```\n",
    "\n",
    "#### Step 4: Execute the Chain\n",
    "\n",
    "Use the chain to perform the multi-step workflow.\n",
    "\n",
    "```python\n",
    "# Execute the chain with a given topic\n",
    "inputs = {\"topic\": \"Artificial Intelligence\"}\n",
    "result = article_summary_chain.call(inputs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Article:\\n\", result[\"article\"])\n",
    "print(\"\\nSummary:\\n\", result[\"summary\"])\n",
    "```\n",
    "\n",
    "### Benefits of Using Chains in LangChain\n",
    "\n",
    "1. **Modularity**: Break down complex tasks into smaller, manageable steps.\n",
    "2. **Reusability**: Reuse individual steps or functions across different chains.\n",
    "3. **Scalability**: Easily add or modify steps in the workflow to handle more complex tasks.\n",
    "4. **Efficiency**: Automate multi-step processes, reducing the need for manual intervention.\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "LangChain supports more advanced features for managing chains, such as branching, conditional steps, and parallel execution. This allows you to build highly sophisticated workflows tailored to your specific needs.\n",
    "\n",
    "By using chains in LangChain, you can streamline complex workflows, making it easier to harness the full power of large language models for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850668f0",
   "metadata": {},
   "source": [
    "### Agents: Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19db4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d22c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba25bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0740ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some tools to use\n",
    "\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"...\"\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b20f48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's initialize an agent with:\n",
    "# 1. The tools\n",
    "# 2. The language model\n",
    "# 3. The type of agent we want to use.\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2cf54",
   "metadata": {},
   "source": [
    "See list of agents types [here](https://python.langchain.com/docs/modules/agents/agent_types/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89728919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Japan is and then calculate the largest prime number that is smaller than their age.\n",
      "Action: Search\n",
      "Action Input: \"current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of the leader of Japan\n",
      "Action: Search\n",
      "Action Input: \"age of Fumio Kishida\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m65 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than 65\n",
      "Action: Calculator\n",
      "Action Input: 65\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 65\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 61.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's test it out!\n",
    "agent.run(\"Who is the current leader of Japan? What is the largest prime number that is smaller than their age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50198449",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Agents in LangChain\n",
    "\n",
    "Agents in LangChain enable dynamic decision-making based on user input or other conditions. Agents can select and execute different chains or functions based on the input they receive, making them highly flexible for building interactive and adaptive applications.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Agent Definition**: Define an agent that can handle different tasks based on user input.\n",
    "2. **Chain Selection**: Use the agent to dynamically select and call the appropriate chain or function.\n",
    "3. **Execution and Response**: Execute the selected chain and return the response.\n",
    "\n",
    "### Example Implementation\n",
    "\n",
    "#### Step 1: Install LangChain\n",
    "\n",
    "First, you need to install the LangChain package if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "#### Step 2: Define Chains\n",
    "\n",
    "Define multiple chains that the agent can dynamically call based on user input. For example, let’s create chains for writing an article and summarizing text.\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.chains import Chain\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Chain for writing an article\n",
    "class WriteArticleChain(Chain):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        topic = inputs[\"topic\"]\n",
    "        template = \"\"\"\n",
    "        Write a detailed article about {topic}. Make sure to include the following points:\n",
    "        1. Introduction to {topic}\n",
    "        2. Key features and benefits of {topic}\n",
    "        3. Examples or case studies related to {topic}\n",
    "        4. Conclusion\n",
    "\n",
    "        The article should be informative and engaging.\n",
    "        \"\"\"\n",
    "        prompt_template = PromptTemplate(template=template)\n",
    "        prompt = prompt_template.format(topic=topic)\n",
    "        article = self.llm.generate(prompt)\n",
    "        return {\"article\": article}\n",
    "\n",
    "# Chain for summarizing text\n",
    "class SummarizeTextChain(Chain):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        text = inputs[\"text\"]\n",
    "        template = \"\"\"\n",
    "        Summarize the following text:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        prompt_template = PromptTemplate(template=template)\n",
    "        prompt = prompt_template.format(text=text)\n",
    "        summary = self.llm.generate(prompt)\n",
    "        return {\"summary\": summary}\n",
    "```\n",
    "\n",
    "#### Step 3: Define the Agent\n",
    "\n",
    "Define an agent that will dynamically select and call the appropriate chain based on user input.\n",
    "\n",
    "```python\n",
    "class TaskAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.write_article_chain = WriteArticleChain(llm)\n",
    "        self.summarize_text_chain = SummarizeTextChain(llm)\n",
    "    \n",
    "    def handle_input(self, user_input, task_type):\n",
    "        if task_type == \"write_article\":\n",
    "            return self.write_article_chain.call({\"topic\": user_input})\n",
    "        elif task_type == \"summarize_text\":\n",
    "            return self.summarize_text_chain.call({\"text\": user_input})\n",
    "        else:\n",
    "            return {\"error\": \"Invalid task type\"}\n",
    "\n",
    "# Initialize the agent\n",
    "agent = TaskAgent(llm=llm)\n",
    "```\n",
    "\n",
    "#### Step 4: Use the Agent to Dynamically Call Chains\n",
    "\n",
    "Use the agent to dynamically call the appropriate chain based on user input and task type.\n",
    "\n",
    "```python\n",
    "# Example user inputs\n",
    "article_topic = \"Artificial Intelligence\"\n",
    "text_to_summarize = \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines...\"\n",
    "\n",
    "# Dynamic call to write an article\n",
    "result_article = agent.handle_input(article_topic, \"write_article\")\n",
    "print(\"Article:\\n\", result_article[\"article\"])\n",
    "\n",
    "# Dynamic call to summarize text\n",
    "result_summary = agent.handle_input(text_to_summarize, \"summarize_text\")\n",
    "print(\"\\nSummary:\\n\", result_summary[\"summary\"])\n",
    "```\n",
    "\n",
    "### Benefits of Using Agents in LangChain\n",
    "\n",
    "1. **Flexibility**: Dynamically select and execute different chains based on user input or conditions.\n",
    "2. **Interactivity**: Build interactive applications that can handle a variety of tasks based on user requests.\n",
    "3. **Scalability**: Easily add new chains and extend the agent’s capabilities.\n",
    "4. **Efficiency**: Automate decision-making and task execution, reducing the need for manual intervention.\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "LangChain supports more advanced features for agents, such as incorporating external APIs, performing complex decision-making, and handling asynchronous tasks. This allows you to build highly interactive and adaptive applications tailored to your specific needs.\n",
    "\n",
    "By using agents in LangChain, you can create dynamic, flexible, and interactive applications that leverage the full potential of large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808dc66",
   "metadata": {},
   "source": [
    "### Memory: Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc3294d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "397ac43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9823cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3d547bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb71a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there!\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d7daf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:  You said \"Hi there!\"\n",
      "Human: what is an alternative phrase for the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative phrase for the first thing you said to me could be \"Greetings!\"'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is an alternative phrase for the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376e776",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "\n",
    "### Memory in LangChain\n",
    "\n",
    "Memory in LangChain allows you to store and manage the state within chains and agents. This can be useful for applications that require context persistence across multiple interactions, such as chatbots, recommendation systems, or personalized content generation.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Memory Management**: Define how the state is stored and retrieved.\n",
    "2. **Stateful Chains**: Chains that can access and update the memory.\n",
    "3. **Stateful Agents**: Agents that use memory to make decisions and manage tasks.\n",
    "\n",
    "### Example Implementation\n",
    "\n",
    "#### Step 1: Install LangChain\n",
    "\n",
    "First, install the LangChain package if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "#### Step 2: Define Memory Management\n",
    "\n",
    "Create a simple memory management system to store and retrieve the state.\n",
    "\n",
    "```python\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.state.get(key, None)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        self.state[key] = value\n",
    "```\n",
    "\n",
    "#### Step 3: Define Stateful Chains\n",
    "\n",
    "Define chains that can use the memory to store and retrieve information.\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.chains import Chain\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Stateful chain for writing an article\n",
    "class StatefulWriteArticleChain(Chain):\n",
    "    def __init__(self, llm, memory):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        topic = inputs[\"topic\"]\n",
    "        template = \"\"\"\n",
    "        Write a detailed article about {topic}. Make sure to include the following points:\n",
    "        1. Introduction to {topic}\n",
    "        2. Key features and benefits of {topic}\n",
    "        3. Examples or case studies related to {topic}\n",
    "        4. Conclusion\n",
    "\n",
    "        The article should be informative and engaging.\n",
    "        \"\"\"\n",
    "        prompt_template = PromptTemplate(template=template)\n",
    "        prompt = prompt_template.format(topic=topic)\n",
    "        article = self.llm.generate(prompt)\n",
    "        \n",
    "        # Store the article in memory\n",
    "        self.memory.set(\"last_article\", article)\n",
    "        \n",
    "        return {\"article\": article}\n",
    "\n",
    "# Stateful chain for summarizing text\n",
    "class StatefulSummarizeTextChain(Chain):\n",
    "    def __init__(self, llm, memory):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        text = inputs.get(\"text\")\n",
    "        \n",
    "        # If no text is provided, summarize the last article from memory\n",
    "        if text is None:\n",
    "            text = self.memory.get(\"last_article\")\n",
    "        \n",
    "        template = \"\"\"\n",
    "        Summarize the following text:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        prompt_template = PromptTemplate(template=template)\n",
    "        prompt = prompt_template.format(text=text)\n",
    "        summary = self.llm.generate(prompt)\n",
    "        \n",
    "        return {\"summary\": summary}\n",
    "```\n",
    "\n",
    "#### Step 4: Define a Stateful Agent\n",
    "\n",
    "Define an agent that can manage multiple stateful chains and use memory.\n",
    "\n",
    "```python\n",
    "class StatefulTaskAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.memory = Memory()\n",
    "        self.llm = llm\n",
    "        self.write_article_chain = StatefulWriteArticleChain(llm, self.memory)\n",
    "        self.summarize_text_chain = StatefulSummarizeTextChain(llm, self.memory)\n",
    "    \n",
    "    def handle_input(self, user_input, task_type):\n",
    "        if task_type == \"write_article\":\n",
    "            return self.write_article_chain.call({\"topic\": user_input})\n",
    "        elif task_type == \"summarize_text\":\n",
    "            return self.summarize_text_chain.call({\"text\": user_input})\n",
    "        else:\n",
    "            return {\"error\": \"Invalid task type\"}\n",
    "\n",
    "# Initialize the stateful agent\n",
    "agent = StatefulTaskAgent(llm=llm)\n",
    "```\n",
    "\n",
    "#### Step 5: Use the Stateful Agent\n",
    "\n",
    "Use the stateful agent to perform tasks and maintain context across interactions.\n",
    "\n",
    "```python\n",
    "# Example user inputs\n",
    "article_topic = \"Artificial Intelligence\"\n",
    "text_to_summarize = \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines...\"\n",
    "\n",
    "# Dynamic call to write an article\n",
    "result_article = agent.handle_input(article_topic, \"write_article\")\n",
    "print(\"Article:\\n\", result_article[\"article\"])\n",
    "\n",
    "# Dynamic call to summarize text\n",
    "# This will summarize the last article if no text is provided\n",
    "result_summary = agent.handle_input(None, \"summarize_text\")\n",
    "print(\"\\nSummary:\\n\", result_summary[\"summary\"])\n",
    "```\n",
    "\n",
    "### Benefits of Adding Memory to Chains and Agents\n",
    "\n",
    "1. **Context Preservation**: Maintain context across multiple interactions, improving the relevance and coherence of responses.\n",
    "2. **Personalization**: Use memory to personalize interactions based on previous user inputs and preferences.\n",
    "3. **Efficiency**: Avoid redundant operations by reusing stored information, reducing the need for repeated computations.\n",
    "4. **Flexibility**: Build more sophisticated applications that can handle complex workflows and dynamic interactions.\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "LangChain supports more advanced memory features, such as long-term memory storage, contextual memory retrieval, and integration with external databases or storage systems. This allows you to build highly interactive and stateful applications tailored to your specific needs.\n",
    "\n",
    "By adding memory to chains and agents in LangChain, you can create dynamic, context-aware applications that leverage the full potential of large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
